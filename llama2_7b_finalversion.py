# -*- coding: utf-8 -*-
"""llama2-7b-FINALVERSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zHhN-KT3VhfytYM2Q-2a4XSGrHrs-85T

### Setting up
"""

!pip install -U bitsandbytes transformers peft==0.4.0 accelerate transformers==4.31.0 trl==0.4.7

import os
import torch
from datasets import load_dataset,load_metric
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer

from huggingface_hub import notebook_login
notebook_login()

# The model from the Hugging Face hub
base_model = "NousResearch/Llama-2-7b-chat-hf"

data ="abirT/ophthalmologydataset"
# Fine-tuned model name
new_model = "Llama-2-7b-ophtalmology"

"""

---

### Data loading

---

"""

# Load the dataset
train_dataset = load_dataset(data, split="train")

train_dataset["text"][50]

print(train_dataset)

"""

---

### Loading the  model

---

"""

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

quant_config= BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map={"": 0}
)
model.config.use_cache = False # silence the warnings
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

print(model)

"""

---

### Loading the Tokenizer

---

"""

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""

---

### Adding the adopter to the layer

---

"""

model = prepare_model_for_kbit_training(model)
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

model = prepare_model_for_kbit_training(model)
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj","gate_proj"]
)
model = get_peft_model(model, peft_config)

print_trainable_parameters(model)

print(model)

"""

---


### Hyperparmeters
---

"""

training_arguments = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=True,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard",
)

"""

---

### SFT parameters
---


"""

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    peft_config=peft_config,
    max_seq_length= max_seq_length,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

"""

---
### Model Training
---



"""

trainer.train()

from tensorboard import notebook
log_dir = "results/runs"
notebook.start("--logdir {} --port 4000".format(log_dir))

"""

---

### Saving the model

---

"""

trainer.model.save_pretrained(new_model)
trainer.tokenizer.save_pretrained(new_model)
model.config.use_cache = True

"""

---

### Pushing the model to the hub

---

"""

trainer.model.push_to_hub(new_model, use_temp_dir=False)

"""

---

### Test the model

---

"""

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",
    max_length=320,
    num_return_sequences=1,
    clean_up_tokenization_spaces=True,
)

# Example prompt for text generation
prompt = (
    "La vascularisation de la rétine est assurée par ? a. La chorio-capillaire assure la vascularisation des couches externes de la rétine. b. La chorio-capillaire assure la vascularisation des couches internes de la rétine. c. L’artère centrale de la rétine assure la vascularisation des couches externes de la rétine. d. L’artère centrale de la rétine assure la vascularisation des couches internes de la rétine. e. La fovéa est vascularisée uniquement par la chorio-capillaire."
)

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] {prompt} [/INST]")[0]['generated_text']
print(generated_text)

"""### =======> Not correct"""

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",
    max_length=200,
    num_return_sequences=1,
    clean_up_tokenization_spaces=True,
)

# Example prompt for text generation
prompt = (
    "La conjonctivite allergique peut se manifester par ? a. Un prurit. b. Une photophobie. c. Un larmoiement. d. Une baisse de l'acuité visuelle. e. Toutes les réponses sont justes."
)

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] {prompt} [/INST]")[0]['generated_text']
print(generated_text)

"""### =========> Not correct"""

from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=200,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Prompt for text generation
prompt = "L’amblyopie fonctionnelle ? a. Est fréquente en cas de strabisme unilatéral. b. Peut être secondaire à une cataracte. c. Peut être secondaire à une anisométropie. d. Son traitement repose sur l’occlusion de l’œil amblyope. e. Son traitement repose sur l'occlusion de l’œil sain."

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] {prompt} [/INST]")[0]['generated_text']
print(generated_text)

"""### =====> Correct"""

# Assuming you already have the model and tokenizer loaded as before
from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=200,  # Adjust max_length as needed
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Example prompt for text generation
prompt = "What is the meaning of Naevus flammeus of the eyelid?"

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] Question: {prompt} [/INST]")[0]['generated_text']
print(generated_text)

from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=200,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Example prompt for text generation
prompt = "Le mélanome de la choroïde ? a. Est la tumeur maligne intraoculaire la plus fréquente chez l'adulte. b. Est la tumeur bénigne intraoculaire la plus fréquente chez l'adulte. c. Est la tumeur maligne intraorbitaire la plus fréquente chez l'enfant. d. Est la tumeur bénigne intraorbitaire la plus fréquente chez l'enfant. e. Est une tumeur d'origine génétique."

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] {prompt} [/INST]")[0]['generated_text']
print(generated_text)

from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=300,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Example prompt for text generation
prompt = "Le glaucome primitif à angle ouvert est caractérisé par ? a. Parmi les facteurs de risque : ATCD familiaux de glaucome, la myopie. d. Est une maladie qui a un traitement radical. e. Se caractérise par un angle irido-cornéen AIC ouvert, un tonus oculaire inférieur à 21 mmHg. ?"

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] {prompt} [/INST]")[0]['generated_text']
print(generated_text)

"""### ========> Correct"""

from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=300,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Example prompt for text generation
prompt = "A 50-year-old man came to the ophthalmologist with the complaint of inability to read newspaper. His vision problem is likely due to inadequate contraction of which of the following structures? Options: A. Ciliary body B. Dilator pupillae C. Extraocular muscles D. Suspensory ligaments of lens"

# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] Question: {prompt} [/INST]")[0]['generated_text']
print(generated_text)

from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=200,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Example prompt for text generation
prompt = "What is the potential cause of conjunctival laceration ?"
# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] {prompt} [/INST]")[0]['generated_text']
print(generated_text)

"""correct"""

from transformers import pipeline

# Text generation pipeline setup
text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=400,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

# Example prompt for text generation
prompt = "Dear MedlinePus I started to fly at the age of 15 and have been flying for over 3,5 years and has logged over 120 hours in my time as pilot. My dream is to become a Comerical Pilot and recently I undertook a medical class 1 which I failed in Norway because of my Eye position and Eye movment, hence I have not experienced any problem with my vision in daily life nor when I have been flying.. The problem is that my binocular vision is not good enough and the eye-doctor meant it could be a problem if I sudden developed double vision. Is there any operation that can fix this? The Orthoptics I went to meant there was a problem with one of the eye muscles which where to tight and the other too weak. She said it can be fixed, but this is not normal ; in Norway because it is not big enough error she meant. Is there anyway to fix this? I appreciate all information! Kind Regards Adam Mauseth"
# Generate text based on prompt
generated_text = text_generation_pipeline(f"<s>[INST] Question: {prompt} [/INST]")[0]['generated_text']
print(generated_text)

"""


---

### Load base model with adopter

---


"""

# Reload model in FP16 and merge it with LoRA weights
base_model_reload = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
model = PeftModel.from_pretrained(base_model_reload, new_model)

"""

---

### Reload tokenizer

---

"""

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""

---
### Inference

---


"""

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer = tokenizer,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

text_generation_pipeline = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    framework="pt",  # PyTorch framework
    max_length=200,
    num_return_sequences=1,  # Generate only one sequence
    clean_up_tokenization_spaces=True,  # Clean up tokenization artifacts
)

prompt = (
    "L’hyphéma est ? a. Un niveau purulent de chambre antérieure. b. Un signe de traumatisme oculaire. c. Une hémorragie de la chambre antérieure. d. Risque de se compliquer d'hémato-cornée ou d'hypertonie oculaire. e. Nécessite dans certains cas un lavage de la chambre antérieure."
)

# Generate text based on prompt
sequences = text_generation_pipeline(
    f"<s>[INST] {prompt} [/INST]",
    do_sample=True,
    max_new_tokens=100,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    num_return_sequences=1,
)

# Print the generated text
print(sequences[0]['generated_text'])

"""


---

### Pushing the fine-tuned model
---

"""

model = model.merge_and_unload()
model.push_to_hub(new_model, use_temp_dir=False)
tokenizer.push_to_hub(new_model, use_temp_dir=False)

"""

---

### Accessing the fine-tuned model

---

"""

from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load the PEFT configuration and base model
config = PeftConfig.from_pretrained("abirT/Llama-2-7B-ophtalmology")
base_model = AutoModelForCausalLM.from_pretrained("NousResearch/Llama-2-7b-chat-hf")

# Apply the PEFT model
model = PeftModel.from_pretrained(base_model, "abirT/Llama-2-7B-ophtalmology")

# Merge PEFT parameters with the base model
model = model.merge_and_unload()

# Save the merged model and tokenizer
model.save_pretrained("transformed-llama-2-7b-ophtalmology")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer.save_pretrained("transformed-llama-2-7b-ophtalmology")

# Push the transformed model to the Hugging Face Hub
model.push_to_hub("Llama-2-7B-ophtalmology-transformers")
tokenizer.push_to_hub("Llama-2-7B-ophtalmology-transformers")

"""
---

### **Accessing the fine-tuned model**


---


To showcase that we can load and run inference without the help of the base model, we will load the fine-tuned model from Hugging Face Hub and run the inference."""

from transformers import pipeline


pipe = pipeline(
    "text-generation",
    model = "abirT/Llama-2-7B-ophtalmology-transformers",
    device_map="auto"
)

prompt = "Question: In retinoblastoma, after enucleation, which tissue is sectioned to find out systemic metastasis Options: A. Central retinal aery B. Sclera and episclera C. Optic nerve D. Voex vein"

sequences = pipe(
    f"<s>[INST] {prompt} [/INST]",
    do_sample=True,
    max_new_tokens=200,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    num_return_sequences=1,)

print(sequences[0]['generated_text'])

prompt = "Inferential specialized clinical ophthalmology procedural question: How was the analysis conducted to determine the location of the dipole source in amblyopia patients ?"

sequences = pipe(
    f"<s>[INST] {prompt} [/INST]",
    do_sample=True,
    max_new_tokens=200,
    temperature=0.9,
    top_k=50,
    top_p=0.95,
    num_return_sequences=1,)

print(sequences[0]['generated_text'])

prompt = "What are the appropriate medicines for the treatment of pigmentary glaucoma ?"

sequences = pipe(
    f"<s>[INST] {prompt} [/INST]",
    do_sample=True,
    max_new_tokens=200,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    num_return_sequences=1,)

print(sequences[0]['generated_text'])

"""

---

### Convert PEFT Model to Transformers Model

---

lload the PEFT (Parameter-Efficient Fine-Tuning) model, save it, and then convert it to a standard Hugging Face Transformers model."""

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# Load the PEFT configuration and base model
config = PeftConfig.from_pretrained("abirT/Llama-2-7B-ophtalmology")
base_model = AutoModelForCausalLM.from_pretrained("NousResearch/Llama-2-7b-chat-hf")

# Apply the PEFT model
model = PeftModel.from_pretrained(base_model, "abirT/Llama-2-7B-ophtalmology")

# Merge PEFT parameters with the base model
model = model.merge_and_unload()

# Save the merged model and tokenizer
model.save_pretrained("transformed-llama-2-7b-ophtalmology")
tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-chat-hf")
tokenizer.save_pretrained("transformed-llama-2-7b-ophtalmology")

# Push the transformed model to the Hugging Face Hub
model.push_to_hub("Llama-2-7B-ophtalmology-transformers")
tokenizer.push_to_hub("Llama-2-7B-ophtalmology-transformers")

"""
---

### *Save the Model with SafeTensors Format*


---

Reload the Saved Transformers Model and Save it with SafeTensors Format.

"""

import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# Load the PEFT configuration and base model
config = PeftConfig.from_pretrained("abirT/Llama-2-7B-ophtalmology")

# Ensure the model is loaded using SafeTensors
base_model = AutoModelForCausalLM.from_pretrained("NousResearch/Llama-2-7b-chat-hf")

# Apply the PEFT model
model = PeftModel.from_pretrained(base_model, "abirT/Llama-2-7B-ophtalmology")

# Merge PEFT parameters with the base model
model = model.merge_and_unload()

# Save the merged model and tokenizer with SafeTensors
save_directory = "transformed-llama-2-7b-ophtalmology"
model.save_pretrained(save_directory, safe_serialization=True, use_safetensors=True)
tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-chat-hf")
tokenizer.save_pretrained(save_directory)

# Verify that the files are saved correctly
files = os.listdir(save_directory)
print(f"Files saved in {save_directory}: {files}")

# Check if SafeTensors files are present
safetensors_files = [f for f in files if f.endswith('.safetensors')]
if safetensors_files:
    print("SafeTensors files found:", safetensors_files)
else:
    print("No SafeTensors files found.")

# Push the transformed model to the Hugging Face Hub with SafeTensors files
try:
    model.push_to_hub("Llama-2-7B-finetuning-ophtalmology", safe_serialization=True)
    print("Model pushed to the Hub successfully.")
except Exception as e:
    print(f"Failed to push the model to the Hub: {e}")

try:
    tokenizer.push_to_hub("Llama-2-7B-finetuning-ophtalmology")
    print("Tokenizer pushed to the Hub successfully.")
except Exception as e:
    print(f"Failed to push the tokenizer to the Hub: {e}")